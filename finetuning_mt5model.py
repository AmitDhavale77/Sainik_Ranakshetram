# -*- coding: utf-8 -*-
"""Finetuning_mT5model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XaQi_-o8Gc7ABiEIkLtIMZirSyhjvoro
"""

#mount gdrive
from google.colab import drive
drive.mount('/content/gdrive')

!pip install transformers #Transformers provides APIs and tools to easily download and train state-of-the-art pretrained models.

from google.colab import drive #to read data from googledrive(not required once drive is mounted)
from IPython.display import display
from IPython.html import widgets #to generate different types of widgets(small parts on web applications that allow user input) and implement them in a notebook.
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns #to visualize data
import torch #open source machine learning (ML) framework based on the Python programming language and the Torch library
from torch import optim # package implementing various optimization algorithms
from torch.nn import functional as F
from transformers import AdamW, AutoModelForSeq2SeqLM, AutoTokenizer
from transformers import get_linear_schedule_with_warmup
from tqdm import tqdm_notebook #library in Python which is used for creating Progress Meters or Progress Bars.
import pandas as pd #to deal with csv files

sns.set()

train_df = pd.read_csv("/content/gdrive/MyDrive/SR_dataset1/hingtraindataset1.csv").astype(str) #take training data as string

train_df.head() #see the train data

train_df1 = train_df.drop(train_df.columns[[0]],axis = 1) #drop the 0th column of table

train_df1.insert(0, 'prefix', 'hing.eng') #insert hing.eng to the 0th column

train_df1=train_df1.reset_index(drop=True)

train_df1.head()

eval_df1 = pd.read_csv("/content/gdrive/MyDrive/SR_dataset1/hingevaldataset1.csv").astype(str)

eval_df1.head()

eval_df = eval_df1.drop(eval_df1.columns[[0]],axis = 1)
eval_df.insert(0, 'prefix', 'hing.eng')
eval_df=eval_df.reset_index(drop=True)
eval_df.head()

model_repo = 'google/mt5-base' #path to repository of pretrained model
#The model google mt5 base is a Natural Language Processing (NLP) Model implemented in Transformer library,
model_path = '/content/gdrive/MyDrive/SR_dataset1/mt5_translation27000_2.pt' #path where trained model is stored
max_seq_len = 40

!pip install sentencepiece #requred to run the command in below cell.After installing restart the kernel

tokenizer = AutoTokenizer.from_pretrained(model_repo) #to take pregenerated tokens from the model

model = AutoModelForSeq2SeqLM.from_pretrained("/content/gdrive/MyDrive/Deepspeech") #to load the original model(here model is downloaded and stored in Deepspeech folder)
model = model.cuda()#to run model using gpu

model.config.max_length=40

len(tokenizer.vocab) #total no. of different tokens

train_df1['prefix'].unique() #take unique strings from 'prefix' column

LANG_TOKEN_MAPPING = {
    'hing.eng': ''
    
}
#A dict which maps hing.eng to ''

special_tokens_dict = {'additional_special_tokens': list(LANG_TOKEN_MAPPING.values())} # to add our extra special token hing.eng=''
special_tokens_dict

tokenizer.add_special_tokens(special_tokens_dict) #here we add our extra special token hing.eng=''

tokenizer.all_special_tokens #all special tokens

model.config.vocab_size #initial vocabsize of model

model.resize_token_embeddings(len(tokenizer)) #change embedding size of model to fit my modeified tokenizer

def encode_str(text, tokenizer, seq_len): #convert a string to tokens
  input_ids = tokenizer.encode(text=text,return_tensors = 'pt',padding = 'max_length',truncation = True,max_length = seq_len)
  return input_ids[0]

t1=encode_str(' tuu ek panchi hain',tokenizer, max_seq_len) #convert words to their tokens
print(t1)
tokens = tokenizer.convert_ids_to_tokens(t1)#convert numbers back to their respective token words
print(tokens)

def encode_row(row, tokenizer, seq_len): #function to encode a row [encode_row()] in the dataset, which calls the encode_string() function.
  """Encode input and target texts from single row"""
  """Returns input and output tensors"""
  input_text = row['Hinglish']
  target_text = row['English']

  if input_text is None or target_text is None:
    return None

  input_token_ids = encode_str(
      input_text, tokenizer, seq_len)
  
  target_token_ids = encode_str(
      target_text, tokenizer, seq_len)

  return input_token_ids, target_token_ids

def encode_batch(batch, tokenizer): #function to encode a batch of rows [encode_batch()] in the dataset, which calls the encode_row() function.

  """Encode a single batch"""
  """Returns input and output batch ids"""
  inputs = []
  targets = []
  for index, row in batch.iterrows():
    #gets input and output tocken ids
    formatted_data = encode_row(
        row, tokenizer, max_seq_len)
#    print("i/o tocken ids:",formatted_data)    
    if formatted_data is None:
      continue
    
    input_ids, target_ids = formatted_data
    #unsqueeze(input, dim) returns a new tensor with a dimension of size one inserted at the specified position
    inputs.append(input_ids.unsqueeze(0))
    targets.append(target_ids.unsqueeze(0))
#  print('squeezed tocken ids:',inputs,targets)

#Concatenate the given sequence of seq tensors in the given dimension    
  batch_input_ids = torch.cat(inputs).cuda()
  batch_target_ids = torch.cat(targets).cuda()

  return batch_input_ids, batch_target_ids

def data_generator(dataset, tokenizer, batch_size=32): #function to generate batches [data_generator()] which calls encode_batch() function.
  """"generates batches"""
  #shuffle the data
  dataset=dataset.sample(frac=1).reset_index(drop=True)
  print(dataset)
  for i in range(0, len(dataset), batch_size):
    raw_batch = dataset[i:i+batch_size]
    print(raw_batch)
    yield encode_batch(raw_batch, tokenizer)
#take batches of batch_size and pass them to batch encoding

# Testing data generator
data_gen = data_generator(train_df1, tokenizer, 8)
data_batch = next(data_gen) #next() function returns the next item in an iterator
#print('data_batch:',data_batch)
print('Input shape:', data_batch[0].shape)
print('Output shape:', data_batch[1].shape)

save_model=model.load_state_dict(torch.load(model_path)) #use only when model is trained and stored in a specific folder

#Initialize hyperparameters
n_epochs = 3
batch_size = 8
print_freq = 1000
checkpoint_freq = 10000
lr = 5e-4 
n_batches = int(np.ceil(len(train_df1) / batch_size)) #number of interations : 3376
total_steps = n_epochs * n_batches 
n_warmup_steps = int(total_steps * 0.01)

int(np.ceil(len(train_df1) / batch_size)) #total number of batches in 1 epoch

optimizer = AdamW(model.parameters(), lr=lr)
# Create a schedule with a learning rate that decreases linearly from the initial lr set in the optimizer to 0, 
# after a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer.
# Warm up steps is a parameter which is used to lower the learning rate 
# in order to reduce the impact of deviating the model from learning on sudden new data set exposure.
scheduler = get_linear_schedule_with_warmup(
    optimizer, n_warmup_steps, total_steps)

def eval_model(model, dataset, max_iters=10): #function to evaluate the model on small data
  """evaluate the model on small test data"""
  test_generator = data_generator(dataset,
                                      tokenizer, batch_size)
  eval_losses = []
  for i, (input_batch, label_batch) in enumerate(test_generator):
    if i >= max_iters:
      break

    model_out = model.forward(
        input_ids = input_batch,
        labels = label_batch)
    eval_losses.append(model_out.loss.item())
    

  return np.mean(eval_losses)

#for training the model
for epoch_idx in range(n_epochs):
  # generate batch data
  data_gen1 = data_generator(train_df,tokenizer, batch_size)
                
  for batch_idx, (input_batch, label_batch) \
      in tqdm_notebook(enumerate(data_gen1), total=n_batches):#tqdm is a library in Python which is used for creating Progress Meters or Progress Bars
    #zeroes all the gradients before the calculation
    optimizer.zero_grad()

    # Forward pass
    model_out = model.forward(
        input_ids = input_batch,
        labels = label_batch)

    # Calculate loss
    loss = model_out.loss
    # loss.item() gets the scalar value held in the loss.  item() turns a Tensor into a Python number
    losses.append(loss.item())
    #backpropagation - computing the gradient of the loss function with respect to each weight
    loss.backward()
    # updating weights to minimize loss
    optimizer.step()
    #You call scheduler.step() every batch, right after optimizer.step(), to update the learning rate.
    scheduler.step()

    # Print training update info
    # every 1000 batches
    if (batch_idx + 1) % print_freq == 0:
      #average loss for the last 1000 batches
      avg_loss = np.mean(losses[-print_freq:])
      print('Epoch: {} | Step: {} | Avg. loss: {:.3f} | lr: {}'.format(
          epoch_idx+1, batch_idx+1, avg_loss, scheduler.get_last_lr()[0]))

    #every 10000 batches save the model 
    if (batch_idx + 1) % checkpoint_freq == 0:
      test_loss = eval_model(model, eval_df)
      print('Saving model with test loss of {:.3f}'.format(test_loss))
      torch.save(model.state_dict(), model_path)

torch.save(model.state_dict(), model_path)

import pickle #to store the loss list
with open('/content/gdrive/MyDrive/SR_dataset1/losses_3.pkl', 'wb') as f:
  pickle.dump(losses, f)

save_model=model.load_state_dict(torch.load(model_path)) #to load pretrained weights from trained model