# -*- coding: utf-8 -*-
"""finalSRcodeipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qv2muSDMGWExP4gE3HEEcbtNkNGoOZYI
"""

#mount gdrive
from google.colab import drive
drive.mount('/content/gdrive')

import os
#change directory to directory in which you will work with audio files
os.chdir('/content/gdrive/MyDrive/amit_sainyaraksh/')

!pip install pydub # Python library to work with only . wav files

from scipy.io import wavfile #to read/write wav files
from pydub import AudioSegment 
import matplotlib.pyplot as plt
# assign files
input_file = "/content/gdrive/MyDrive/amit_sainyaraksh/221018_0559.mp3"   #initial mp3 file
output_file = "/content/gdrive/MyDrive/amit_sainyaraksh/221018_0559.wav"  #path of wav file to be stored after converion from mp3
# convert mp3 file to wav file
# sound = AudioSegment.from_mp3(input_file)
# sound.export(output_file, format="wav")

sound = AudioSegment.from_mp3(input_file) #read mp3 file
sound.export(output_file, format="wav")  #to convert mp3 to wav

from IPython.display import clear_output 
#to clear the output of a cell.
clear_output()

import numpy as np   
from IPython.display import Audio
from scipy.io import wavfile

# Load the file on an object
data = wavfile.read(output_file)
# Separete the object elements
framerate = data[0]
sounddata = data[1]
time      = np.arange(0,len(sounddata))/framerate

# Show information about the object
print('Sample rate:',framerate,'Hz')
print('Total time:',len(sounddata)/framerate,'s')

!pip install playsound #cross platform module that can play audio files
!pip install pygobject #It provides good and consistent access to the entire GNOME software platform with the help of GObject Introspection

import librosa
x, sr = librosa.load(output_file) #load input audio file

# Commented out IPython magic to ensure Python compatibility.
print(x.shape)
print(sr) #see its sampling rate
# %matplotlib inline
import matplotlib.pyplot as plt
import librosa.display
plt.figure(figsize=(14, 5))
librosa.display.waveplot(x, sr=sr) #plot amplitude vs time

# Commented out IPython magic to ensure Python compatibility.
# %load_ext autoreload
# %autoreload 2
#%env CUDA_VISIBLE_DEVICES=3
colab_requirements = [
    "pip install librosa",
    "pip install noisereduce",
    "pip install soundfile",

]

import sys, subprocess

def run_subprocess_command(cmd): #to pip install the dependencies in colab_requirements
    # run the command
    process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE)
    # print the output
    for line in process.stdout:
        print(line.decode().strip())

IN_COLAB = "google.colab" in sys.modules
if IN_COLAB:
    for i in colab_requirements:
        run_subprocess_command(i)

# Commented out IPython magic to ensure Python compatibility.
import IPython #It provides a powerful interface to the Python language.
from scipy.io import wavfile
import noisereduce as nr #It is a noise reduction algorithm in python that reduces noise in time-domain signals like speech, bioacoustics, and physiological signals by using Spectral Grating
import soundfile as sf #used for reading and writing audio files,
from noisereduce.generate_noise import band_limited_noise #to limit noise upto certain range of frequencies
import matplotlib.pyplot as plt #for plotting graphs
#import urllib.request
import numpy as np #to deal with arrays
import io #It allows us to manage the file-related input and output operations
# %matplotlib inline
from scipy.io import wavfile

#url = "https://raw.githubusercontent.com/timsainb/noisereduce/master/assets/cafe_short.wav"
#response = urllib.request.urlopen(url)
#noise_data, noise_rate = sf.read(io.BytesIO(response.read()))

reduced_noise = nr.reduce_noise(y = x, sr=sr, thresh_n_mult_nonstationary=1.5,stationary=False) #Reduce noise from audiofile by spectral grating

reduced_noise

x

# Commented out IPython magic to ensure Python compatibility.
print(reduced_noise.shape)
print(sr)
# %matplotlib inline
import matplotlib.pyplot as plt
import librosa.display
plt.figure(figsize=(14, 5))
librosa.display.waveplot(reduced_noise, sr=sr) #plotting noise reduced version of input audio

!pip install transformers #Transformers provides APIs and tools to easily download and train state-of-the-art pretrained models.

from google.colab import drive #to read data from googledrive(not required once drive is mounted)
from IPython.display import display
from IPython.html import widgets #to generate different types of widgets(small parts on web applications that allow user input) and implement them in a notebook.
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns #to visualize data
import torch #open source machine learning (ML) framework based on the Python programming language and the Torch library
from torch import optim # package implementing various optimization algorithms
from torch.nn import functional as F
from transformers import AdamW, AutoModelForSeq2SeqLM, AutoTokenizer
from transformers import get_linear_schedule_with_warmup
from tqdm import tqdm_notebook #library in Python which is used for creating Progress Meters or Progress Bars.
import pandas as pd #to deal with csv files

sns.set()

train_df = pd.read_csv("/content/gdrive/MyDrive/SR_dataset1/hingtraindataset1.csv").astype(str) #take training data as string

train_df.head() #see the train data

train_df1 = train_df.drop(train_df.columns[[0]],axis = 1) #drop the oth column of table

train_df1.insert(0, 'prefix', 'hing.eng') #insert hing.eng to the 0th column

train_df1=train_df1.reset_index(drop=True)

train_df1.head()

eval_df1 = pd.read_csv("/content/gdrive/MyDrive/SR_dataset1/hingevaldataset1.csv").astype(str)

eval_df1.head()

eval_df = eval_df1.drop(eval_df1.columns[[0]],axis = 1)
eval_df.insert(0, 'prefix', 'hing.eng')
eval_df=eval_df.reset_index(drop=True)
eval_df.head()

model_repo = 'google/mt5-base' #path to repository of pretrained model
#The model google mt5 base is a Natural Language Processing (NLP) Model implemented in Transformer library,
model_path = '/content/gdrive/MyDrive/SR_dataset1/mt5_translation27000_2.pt' #path where trained model is stored
max_seq_len = 40

!pip install sentencepiece #requred to run the command in below cell.After installing restart the kernel

tokenizer = AutoTokenizer.from_pretrained(model_repo) #to take pregenerated tokens from the model

model = AutoModelForSeq2SeqLM.from_pretrained("/content/gdrive/MyDrive/Deepspeech") #to load the original model(here model is downloaded and stored in Deepspeech folder)
model = model.cuda()#to run model using gpu

model.config.max_length=40

len(tokenizer.vocab) #total no. of different tokens

train_df1['prefix'].unique()

LANG_TOKEN_MAPPING = {
    'hing.eng': ''
    
}
#A dict which maps hing.eng to ''

special_tokens_dict = {'additional_special_tokens': list(LANG_TOKEN_MAPPING.values())} #add our extra special token hing.eng=''
special_tokens_dict

tokenizer.add_special_tokens(special_tokens_dict)

tokenizer.all_special_tokens #all special tokens

model.config.vocab_size #initial vocabsize of model

model.resize_token_embeddings(len(tokenizer)) #change embedding size of model to fit my modeified tokenizer

def encode_str(text, tokenizer, seq_len): #convert a string to tokens
  input_ids = tokenizer.encode(text=text,return_tensors = 'pt',padding = 'max_length',truncation = True,max_length = seq_len)
  return input_ids[0]

save_model=model.load_state_dict(torch.load(model_path)) #to load pretrained weights from trained model

import pickle
with open('/content/gdrive/MyDrive/SR_dataset1/losses_2.pkl', 'rb') as f:

  losses1 = pickle.load(f) #to load the file containing losses for the pretrained folder

import gc
#del model
gc.collect() #Use this method to force the system to try to reclaim the maximum amount of available memory.

torch.cuda.empty_cache()

window_size = 50
smoothed_losses = []
for i in range(len(losses1)-window_size):
  smoothed_losses.append(np.mean(losses1[i:i+window_size]))#plot mean of each windowed size loss

plt.plot(smoothed_losses[100:])

!pip install sacrebleu #used to produce BLEU(bilingual evaluation understudy) scores
import sacrebleu

english_truth = [eval_df.loc[eval_df["prefix"] == "hing.eng"]["English"].tolist()] #actual test values

english_truth = english_truth[0][:500] #take 1st 500 datas

to_english=eval_df.loc[eval_df["prefix"] == "hing.eng"]["Hinglish"].tolist() #take input to model for checking bleu score
to_english=to_english[:500]

english_preds=[] #stores predicted output 
for i in to_english:
  input_ids = encode_str(
    text = i,
    tokenizer = tokenizer,
    seq_len = model.config.max_length)
  input_ids = input_ids.unsqueeze(0).cuda()
  #print(input_ids)
  output_tokens = model.generate(input_ids, num_beams=10, num_return_sequences=1, length_penalty = 1, no_repeat_ngram_size=2)
  for token_set in output_tokens:
    english_preds.append(tokenizer.decode(token_set, skip_special_tokens=True))

hing_eng_bleu = sacrebleu.corpus_bleu(english_preds, [english_truth]) #to calculate bleu score
print("--------------------------")
print("Hinglish to English: ", hing_eng_bleu.score) #it tells me by how much my overall predicted texts are similer to input texts

english_preds2=[] #to store output for random input

input_ids = encode_str(text = 'tumhara naam kya hain',tokenizer = tokenizer,seq_len = model.config.max_length)
input_ids = input_ids.unsqueeze(0).cuda()
  #print(input_ids)
output_tokens = model.generate(input_ids, num_beams=10, num_return_sequences=1, length_penalty = 1, no_repeat_ngram_size=2)
for token_set in output_tokens:
  english_preds2.append(tokenizer.decode(token_set, skip_special_tokens=True))

english_preds2[0]

!pip install SpeechRecognition pydub

english_preds1=[]

import os #used to deal with directories
import pydub
from pydub.playback import play
from pydub import AudioSegment
from pydub.silence import split_on_silence #split audio file where silence is there 
import scipy #is a collection of mathematical algorithms and convenience functions built on the NumPy extension of Python

#Using Google's speech_recognition
#Here noise reduction is not performed as the accuracy of the predictions by speech_recognition decrease substantially after removing the noise

# importing libraries 
import speech_recognition as sr 
import os 
from pydub import AudioSegment
from pydub.silence import split_on_silence

# create a speech recognition object
r = sr.Recognizer()

# a function that splits the audio file into chunks
# and applies speech recognition
def get_large_audio_transcription(path):
    """
    Splitting the large audio file into chunks
    and apply speech recognition on each of these chunks
    """
    # open the audio file using pydub
    sound = AudioSegment.from_wav(path)  
    # split audio sound where silence is 700 miliseconds or more and get chunks
    chunks = split_on_silence(sound,
        # experiment with this value for your target audio file
        min_silence_len = 1000,
        # adjust this per requirement
        silence_thresh = sound.dBFS-14,
        # keep the silence for 1 second, adjustable as well
        keep_silence=1000,
    )
    folder_name = "audio-chunks4_n"
    # create a directory to store the audio chunks
    if not os.path.isdir(folder_name):
        os.mkdir(folder_name)
    whole_text = ""
    # process each chunk 
    for i, audio_chunk in enumerate(chunks, start=1):
        # export audio chunk and save it in
        # the `folder_name` directory.
        chunk_filename = os.path.join(folder_name, f"chunk{i}.wav")
        audio_chunk.export(chunk_filename, format="wav")
        wav_file =  pydub.AudioSegment.from_file(file = chunk_filename,format = "wav") #read the audio file
        new_wav_file = wav_file + 10 #increase its volume by 10db
        chunk_filename1 = os.path.join(folder_name, f"chunk_high{i}.wav")
        new_wav_file.export(out_f =  chunk_filename1,format = "wav")
        # recognize the chunk
        with sr.AudioFile(chunk_filename1) as source:
            audio_listened = r.record(source)
            # try converting it to text
            try:
                text = r.recognize_google(audio_listened,language="en-US")
            except sr.UnknownValueError as e:
                print("Error:", str(e))
            else:
                text1 = f"{text.capitalize()}. "
                print(chunk_filename, ":", text)
                whole_text += text1
                input_ids = encode_str(text = text1,tokenizer = tokenizer,seq_len = model.config.max_length)
                input_ids = input_ids.unsqueeze(0).cuda()
                #print(input_ids)
                output_tokens = model.generate(input_ids, num_beams=10, num_return_sequences=1, length_penalty = 1, no_repeat_ngram_size=2)
                for token_set in output_tokens:
                  english_preds1.append(tokenizer.decode(token_set, skip_special_tokens=True)) #contains predicted output by model
    # return the text for all chunks detected
    return whole_text

textdata=get_large_audio_transcription(output_file) #contains speech recognized text

#By using whisper

!pip install git+https://github.com/openai/whisper.git

import whisper

model1 = whisper.load_model("large")

!pip install simpleaudio

import simpleaudio as sa #The simplaudio package provides cross-platform, dependency-free audio playback capability
from pydub import AudioSegment
from pydub.utils import make_chunks
import os
import scipy

import os 
from pydub import AudioSegment
#from pydub.silence import split_on_silence

# create a speech recognition object


# a function that splits the audio file into chunks
# and applies speech recognition
def get_large_audio_transcription(path):
    """
    Splitting the large audio file into chunks
    and apply speech recognition on each of these chunks
    """
    # open the audio file using pydub
    sound = AudioSegment.from_wav(path)  
    # split audio sound where silence is 700 miliseconds or more and get chunks
    chunk_length_ms = 30000
    chunks=make_chunks(sound, chunk_length_ms)
    folder_name = "audio-chunks_whisper"
    # create a directory to store the audio chunks
    if not os.path.isdir(folder_name):
        os.mkdir(folder_name)
    whole_text = ""
    # process each chunk 
    for i, audio_chunk in enumerate(chunks, start=1):
        # export audio chunk and save it in
        # the `folder_name` directory.
        chunk_filename = os.path.join(folder_name, f"chunk{i}.wav")
        audio_chunk.export(chunk_filename, format="wav")
        data, rate = librosa.load(chunk_filename)
        reduced_noise = nr.reduce_noise(y = data, sr=rate, thresh_n_mult_nonstationary=1.5,stationary=False) #apply noise reduction on audio file
        chunk_filename1 = os.path.join(folder_name, f"chunk_nr{i}.wav")
        scipy.io.wavfile.write(chunk_filename1, rate, reduced_noise) #store the noise reduced chunk
        wav_file =  pydub.AudioSegment.from_file(file = chunk_filename1,format = "wav")
        new_wav_file = wav_file + 10 #increase its volume by 10db
        chunk_filename2 = os.path.join(folder_name, f"chunk_nr_high{i}.wav")
        new_wav_file.export(out_f =  chunk_filename2,format = "wav")
        # recognize the chunk
        audio = whisper.load_audio(chunk_filename2)
        audio = whisper.pad_or_trim(audio)
        # make log-Mel spectrogram and move to the same device as the model
        mel = whisper.log_mel_spectrogram(audio).to(model1.device)
        # detect the spoken language
        _, probs = model1.detect_language(mel)
        print(f"Detected language: {max(probs, key=probs.get)}")
        options = whisper.DecodingOptions(language="en", without_timestamps=False) #to decode the sentence in English
        result = whisper.decode(model1, mel, options)
        text1=result.text
        whole_text += text1

        input_ids = encode_str(text = text1,tokenizer = tokenizer,seq_len = model.config.max_length)
        input_ids = input_ids.unsqueeze(0).cuda()
  #print(input_ids)
        output_tokens = model.generate(input_ids, num_beams=10, num_return_sequences=1, length_penalty = 1, no_repeat_ngram_size=2)
        for token_set in output_tokens:
            english_preds1.append(tokenizer.decode(token_set, skip_special_tokens=True))
    return whole_text

textdata=get_large_audio_transcription(output_file) #contains whisper recognized text

textdata

def listToString(s):
 
    # initialize an empty string
    str1 = ""
 
    # traverse in the string
    for ele in s:
        str1 += ele
 
    # return string
    return str1
 
 

x1=listToString(english_preds1)
x1